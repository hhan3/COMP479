{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhan3/COMP479/blob/main/Copy_of_01_ml_notebook_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data-driven Aspects\n",
        "\n",
        "---\n",
        "Loyola University Chicago  \n",
        "COMP 379-001/479-001, Spring 2024, Machine Learning  \n",
        "Instructor: Daniel Moreira (dmoreira1@luc.edu)  \n",
        "More at https://danielmoreira.github.io/teaching/ml-spr24/\n",
        "\n",
        "---\n",
        "\n",
        "Practical examples and exercises of the data-driven aspects of Machine Learning.\n",
        "\n",
        "Language: Python 3  \n",
        "\n",
        "Needed libraries:\n",
        "* NumPy (https://numpy.org/)\n",
        "* Pandas (https://pandas.pydata.org/)\n",
        "* Scikit-learn (https://scikit-learn.org/)"
      ],
      "metadata": {
        "id": "DocA0VkJ4yOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------\n",
        "## Data Partition"
      ],
      "metadata": {
        "id": "w6hJKwFONZ4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the wine dataset\n",
        "!curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
      ],
      "metadata": {
        "id": "bbeauT8POE77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# loads the wine dataset into memory\n",
        "df_wine = pd.read_csv('/content/wine.data')\n",
        "\n",
        "# adds headers to the dataset according to documentation\n",
        "df_wine.columns = [\n",
        "    'label', 'alcohol', 'malic acid', 'ash', 'alcalinity', 'magnesium',\n",
        "    'phenols', 'flavanoids', 'nonflavanoid phenols', 'proanthocyanins',\n",
        "    'color intensity', 'hue', 'protein concentration', 'proline']\n",
        "\n",
        "# prints info\n",
        "print('Data shape:', df_wine.shape)\n",
        "print('Labels, Label count:', np.unique(df_wine['label'], return_counts=True))\n",
        "print()\n",
        "\n",
        "# first 10 samples\n",
        "df_wine.head(10)"
      ],
      "metadata": {
        "id": "4zhHneisPSmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data partition using sklearn\n",
        "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# separation into data (X) and respective labels (y)\n",
        "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
        "print('First 10 samples:', X[:10])\n",
        "print('First 10 labels:', y[:10])\n",
        "print()\n",
        "\n",
        "# split configuration\n",
        "test_size = 0.3 # data percentage going to test\n",
        "random_seed = 0 # save the seed for reproducibility\n",
        "\n",
        "# data split\n",
        "X_train, X_test, y_train, y_test =\\\n",
        "  train_test_split(X, y,\n",
        "                   random_state=random_seed,\n",
        "                   test_size=test_size,\n",
        "                   stratify=y)\n",
        "\n",
        "# train info\n",
        "print('Train data shape:', X_train.shape)\n",
        "print('Train data labels, label count:', np.unique(y_train, return_counts=True))\n",
        "print()"
      ],
      "metadata": {
        "id": "oljCpy7_UX3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "Print info about the test partition."
      ],
      "metadata": {
        "id": "66E78nD4bdwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n"
      ],
      "metadata": {
        "id": "BE8TQ7DCbt6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------\n",
        "## Data Preprocessing\n"
      ],
      "metadata": {
        "id": "UbeYBKEVbxb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Data"
      ],
      "metadata": {
        "id": "wmoYL1tFgDqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the California housing dataset\n",
        "!pip install gdown\n",
        "!gdown 1QkyFNJR8CloAprShjVz7QR8L0e1UuJOf"
      ],
      "metadata": {
        "id": "xioCPzSAgInA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# loads the housing dataset into memory\n",
        "df_housing = pd.read_csv('/content/housing.csv')\n",
        "\n",
        "# prints info\n",
        "print('Data shape:', df_housing.shape)\n",
        "print('All features numerical but last one.')\n",
        "\n",
        "# last 5 samples\n",
        "df_housing.tail(5)"
      ],
      "metadata": {
        "id": "HAhlmbC2h-NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2\n",
        "By using the official [Scikit-learn reference](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), split the California housing dataset into *train* and *test*, with the test partition containing half of the data."
      ],
      "metadata": {
        "id": "I6dHNugcjZkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n"
      ],
      "metadata": {
        "id": "53MVWaA9ijgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ordinal Data"
      ],
      "metadata": {
        "id": "g2kAfTBslY5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy-case example\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame([['green', 'M', 10.1],\n",
        "                   ['red', 'L', 13.5],\n",
        "                   ['blue', 'XL', 15.3],\n",
        "                   ['green', 'S', 8.9]])\n",
        "\n",
        "df.columns = ['color', 'size', 'price']\n",
        "df"
      ],
      "metadata": {
        "id": "WcoZBNeklmpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ordinal data mapping to numerical\n",
        "size_mapping = {'XL': 3,\n",
        "                'L': 2,\n",
        "                'M': 1,\n",
        "                'S': 0}\n",
        "\n",
        "df['size'] = df['size'].map(size_mapping)\n",
        "df"
      ],
      "metadata": {
        "id": "1s6hbq0SmHfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inverse mapping, to allow further explanation\n",
        "inv_size_mapping = {v: k for k, v in size_mapping.items()}\n",
        "df['size'].map(inv_size_mapping)"
      ],
      "metadata": {
        "id": "rImMCDV8mdgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nominal Data"
      ],
      "metadata": {
        "id": "vVqBvmpfnDRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding example but with colinearity\n",
        "\n",
        "# needed libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# column transformation\n",
        "X = df.values\n",
        "encoder = OneHotEncoder()\n",
        "transformer = ColumnTransformer([('onehot', encoder, [0]), #[0]: color column\n",
        "                               ('keep', 'passthrough', [1, 2])])\n",
        "X_transf = transformer.fit_transform(X).astype(float)\n",
        "print(X_transf)\n",
        "\n",
        "# pandas format\n",
        "df_transf = pd.DataFrame(X_transf)\n",
        "df_transf.columns = ['color_blue', 'color_green', 'color_red', 'size', 'price']\n",
        "df_transf"
      ],
      "metadata": {
        "id": "AXOR7u6onedV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 3\n",
        "\n",
        "**Question**\n",
        "\n",
        "What is the problem with this solution in terms of *colinearity*?\n",
        "\n",
        "> *Add your answer here.*\n",
        ">"
      ],
      "metadata": {
        "id": "0pmDTXYLtD6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding example WITHOUT colinearity\n",
        "\n",
        "# needed libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# column transformation\n",
        "X = df.values\n",
        "encoder = OneHotEncoder(categories='auto', drop='first') # attention here!\n",
        "transformer = ColumnTransformer([ ('onehot', encoder, [0]), #[0]: color column\n",
        "                               ('keep', 'passthrough', [1, 2])])\n",
        "X_transf = transformer.fit_transform(X).astype(float)\n",
        "print(X_transf)\n",
        "\n",
        "# pandas format\n",
        "df_transf = pd.DataFrame(X_transf)\n",
        "df_transf.columns = ['color_green', 'color_red', 'size', 'price']\n",
        "df_transf"
      ],
      "metadata": {
        "id": "ZXnzfKyeteaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 4\n",
        "\n",
        "**Question**\n",
        "\n",
        "How do you say if a shirt is blue?\n",
        "\n",
        "> *Add your answer here.*\n",
        ">"
      ],
      "metadata": {
        "id": "xhWt-ylbuahr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 5\n",
        "Handle the \"*ocean proximity*\" column within the California dataset **train partition**."
      ],
      "metadata": {
        "id": "Yz6wHw_XvnKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n"
      ],
      "metadata": {
        "id": "pogOHk4B0Nm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 6\n",
        "Handle the \"*ocean proximity*\" column within the California dataset **test partition**.\n",
        "\n",
        "> Don't contaminate the data!"
      ],
      "metadata": {
        "id": "V1ujyJi72QJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n"
      ],
      "metadata": {
        "id": "KAumNAjC2V78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 7\n",
        "\n",
        "**Question**\n",
        "\n",
        "How did you avoid contaminating the test set?\n",
        "\n",
        "> *Add your answer here.*\n",
        ">"
      ],
      "metadata": {
        "id": "BjBuCmyx3KmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Data"
      ],
      "metadata": {
        "id": "Lt-CZ-6GVG3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy-case example\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "csv_data = \\\n",
        "'''A,B,C,D,E,F,G\n",
        "1.0,2.0,3.0,4.0,5.0,6.0,7.0\n",
        "8.0,9.0,,11.0,,13.0,14.0\n",
        "15.0,16.0,17.0,,,,21.0\n",
        "22.0,23.0,24.0,25.0,26.0,27.0,28.0'''\n",
        "\n",
        "df = pd.read_csv(StringIO(csv_data))\n",
        "print(df.values)\n",
        "print()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "I1fGAZmnZOP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Drop"
      ],
      "metadata": {
        "id": "4n5wj7YkecUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns that have one or more missing values\n",
        "df_transf = df.dropna(axis=1)\n",
        "df_transf"
      ],
      "metadata": {
        "id": "Liz9uhfRbkqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns that have fewer than <3> non-empty values\n",
        "# i.e., all columns must have at least 3 non-empty values\n",
        "df_transf = df.dropna(axis=1, thresh=3)\n",
        "df_transf"
      ],
      "metadata": {
        "id": "eZV0vzVsbxeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample Drop\n"
      ],
      "metadata": {
        "id": "SGsKkP96ij48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows that have one or more missing values\n",
        "df_transf = df.dropna(axis=0)\n",
        "df_transf"
      ],
      "metadata": {
        "id": "KG6SsAE9i61j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows that have fewer than <5> non-empty values\n",
        "# i.e., all rows must have at least 5 non-empty values\n",
        "df_transf = df.dropna(axis=0, thresh=5)\n",
        "df_transf"
      ],
      "metadata": {
        "id": "Of7YVBX7jG9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows whose specific columns have missing values\n",
        "df_transf = df.dropna(subset=['D', 'F']) # D and F cannot have missing values\n",
        "df_transf"
      ],
      "metadata": {
        "id": "xWKyZharjX6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Value Replacement"
      ],
      "metadata": {
        "id": "8mfJk4_Jt6GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# original values, for reference\n",
        "df"
      ],
      "metadata": {
        "id": "4MiNa2xOy2Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replaces missing values with column-wise mean values\n",
        "mean = df.mean()\n",
        "df_transf = df.fillna(mean)\n",
        "df_transf"
      ],
      "metadata": {
        "id": "_U-kxHGayBNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replaces columns C and D with mean, and E and F with max\n",
        "mean = df.mean()\n",
        "max = df.max()\n",
        "\n",
        "df_transf = df.copy()\n",
        "df_transf[['C', 'D']] = df[['C', 'D']].fillna(mean)\n",
        "df_transf[['E', 'F']] = df[['E', 'F']].fillna(max)\n",
        "df_transf"
      ],
      "metadata": {
        "id": "XZ3lMueMypjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Normalization"
      ],
      "metadata": {
        "id": "7rqWx--GZ5id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Scaling"
      ],
      "metadata": {
        "id": "rSNyJlVgaBiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy-case example (with train and test partition)\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.DataFrame([[0.15, 1230, 0.00000005, 315.10],\n",
        "                         [0.12, 4217, 0.00000027, 117.00],\n",
        "                         [0.23,  943, 0.00000003, 230.40],\n",
        "                         [0.18, 1014,        0.0,   3.14]])\n",
        "\n",
        "df_test = pd.DataFrame([[0.25, 1500, 0.00000002,   3.14],\n",
        "                        [0.16, 3500,        0.0, 100.00]])\n",
        "\n",
        "df_train, df_test"
      ],
      "metadata": {
        "id": "eOxg_0xRaFYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data scale implementation\n",
        "df_max = df_train.max()\n",
        "df_min = df_train.min()\n",
        "\n",
        "df_train_norm = (df_train - df_min) / (df_max - df_min)\n",
        "df_train_norm"
      ],
      "metadata": {
        "id": "fi17vvb4b6Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data scale normalization\n",
        "df_max = df_train.max() # use the train data to avoid contamination!\n",
        "df_min = df_train.min() # use the train data to avoid contamination!\n",
        "\n",
        "df_test_norm = (df_test - df_min) / (df_max - df_min)\n",
        "df_test_norm"
      ],
      "metadata": {
        "id": "SEbPN1rme9nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn data scale implementation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# train data\n",
        "X_train = df_train.values\n",
        "X_train_norm = scaler.fit_transform(X_train) # use fit only on the train set!\n",
        "\n",
        "X_train_norm"
      ],
      "metadata": {
        "id": "s5d1_NzncS1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data\n",
        "X_test = df_test.values\n",
        "X_test_norm = scaler.transform(X_test) # don't use fit on the test set!\n",
        "\n",
        "X_test_norm"
      ],
      "metadata": {
        "id": "HLK9WBB-g5rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Standardization"
      ],
      "metadata": {
        "id": "sihdhLBzhWa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same previous toy-case example (with train and test partition)\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.DataFrame([[0.15, 1230, 0.00000005, 315.10],\n",
        "                         [0.12, 4217, 0.00000027, 117.00],\n",
        "                         [0.23,  943, 0.00000003, 230.40],\n",
        "                         [0.18, 1014,        0.0,   3.14]])\n",
        "\n",
        "df_test = pd.DataFrame([[0.25, 1500, 0.00000002,   3.14],\n",
        "                        [0.16, 3500,        0.0, 100.00]])\n",
        "\n",
        "df_train, df_test"
      ],
      "metadata": {
        "id": "XphQfFbHjzCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data standardization\n",
        "df_mean = df_train.mean()\n",
        "df_std = df_train.std(ddof=0)\n",
        "\n",
        "df_train_norm = (df_train - df_mean) / df_std\n",
        "df_train_norm"
      ],
      "metadata": {
        "id": "FuTkgqW2kO9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data standardization\n",
        "df_mean = df_train.mean()     # use the train data to avoid contamination!\n",
        "df_std = df_train.std(ddof=0) # use the train data to avoid contamination!\n",
        "\n",
        "df_test_norm = (df_test - df_mean) / df_std\n",
        "df_test_norm"
      ],
      "metadata": {
        "id": "DInqyLkIkvA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn implementation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# train data\n",
        "X_train = df_train.values\n",
        "X_train_norm = scaler.fit_transform(X_train) # use fit only on the train set!\n",
        "\n",
        "X_train_norm"
      ],
      "metadata": {
        "id": "yuIZh-hVlIls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data\n",
        "X_test = df_test.values\n",
        "X_test_norm = scaler.transform(X_test) # don't use fit on the test set!\n",
        "\n",
        "X_test_norm"
      ],
      "metadata": {
        "id": "99Fo73mIlohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 8\n",
        "\n",
        "Execute data partition and preprocessing on the California housing dataset, which was provided above (see the \"Numerical Data\" section).   \n",
        "\n",
        "Split the data into train and test partitions, with the test set containing 30% of the data.\n",
        "\n",
        "Handle all the eventual categorical data, and replace all the missing values with the median of their respective features. Lastly, normalize the dataset with standardization.\n",
        "\n",
        "Make sure not to mix train and test partitions.   \n",
        "**Do not contaminate the data!**\n"
      ],
      "metadata": {
        "id": "ZUnAxJsp0d4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n"
      ],
      "metadata": {
        "id": "biHQMjxY5iR-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}